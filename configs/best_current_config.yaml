trainer:
  accelerator: auto
  default_root_dir: 'runs/noise_fft2channels_dmodel18_layers6_lr1e-3_gamma0.99_batchsize256_longrun_12000points_test/'
  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        monitor: val/loss
        mode: min
        save_last: true
    - class_path: lightning.pytorch.callbacks.EarlyStopping
      init_args:
        monitor: val/loss
        patience: 10
        mode: min
  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
  log_every_n_steps: 1
  max_epochs: 80

data:
  class_path: data.LitDataModule
  init_args:
    inputs: ['output_ts_I', 'output_ts_Q']
    variables: ['energy_eV', 'pitch_angle_deg']
    observables: ['avg_axial_frequency_Hz', 'avg_carrier_frequency_Hz', 'radius_m']
    batch_size: 256
    num_workers: 4
    path: '/gpfs/gibbs/pi/heeger/hb637/ssm_files_pi_heeger/combined_data_fullsim.hdf5'
    cutoff: 12000
    noise_const: 1.0
    #apply_fft: True
    #complex_channels: True
    #ts_and_fft: True

optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 1e-3

lr_scheduler:
  class_path: torch.optim.lr_scheduler.ExponentialLR
  init_args:
    gamma: 0.99

model:
  class_path: model.LitS4Model_FFT
  init_args:
    encoder:
        class_path: models.networks.S4DModel
        init_args:
           d_input: 2
           d_output: 2
           d_model: 18
           n_layers: 6
           dropout: 0.0
           prenorm: False
           fc_hidden: []
